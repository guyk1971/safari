{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero Scrolls\n",
    "The goal of this notebook is to explore the zero scrolls dataset.  \n",
    "there are 2 versions: \n",
    "- public version \n",
    "- LC team's version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from transformers import set_seed as hf_set_seed\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting accelerate\n",
      "  Obtaining dependency information for accelerate from https://files.pythonhosted.org/packages/d9/92/2d3aecf9f4a192968035880be3e2fc8b48d541c7128f7c936f430d6f96da/accelerate-0.23.0-py3-none-any.whl.metadata\n",
      "  Downloading accelerate-0.23.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.22.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.4)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0a0+32f93b1)\n",
      "Requirement already satisfied: huggingface-hub in /home/gkoren/.local/lib/python3.10/site-packages (from accelerate) (0.17.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.12.4)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.7.1)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.6.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Downloading accelerate-0.23.0-py3-none-any.whl (258 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.1/258.1 kB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece, accelerate\n",
      "\u001b[33m  WARNING: The scripts accelerate, accelerate-config, accelerate-estimate-memory and accelerate-launch are installed in '/home/gkoren/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed accelerate-0.23.0 sentencepiece-0.1.99\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece accelerate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to run the hyena chckpoint, you need to install dropout_add_layer_norm from flash attention module\n",
    "# see this issue : https://github.com/HazyResearch/hyena-dna/issues/5\n",
    "\n",
    "cd flash-attention/csrc/layer_norm\n",
    "!pip install .\n",
    "cd ../../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd flash-attention/csrc/fused_dense_lib\n",
    "# ! pip install ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ZeroScrolls public\n",
    "Benchmark [Homepage](https://www.zero.scrolls-benchmark.com/) and [github](https://github.com/tau-nlp/zero_scrolls/tree/main)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming the files have been downloaded and located in the `datasets` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zc_data_path='./datasets/zero-scrolls/public/'\n",
    "zc_tasks = os.listdir(zc_data_path)\n",
    "zc_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zc_task=zc_tasks[4]\n",
    "# load from  file\n",
    "# data = load_dataset('json',data_files={'test':os.path.join(zc_data_path,zc_task,'test.jsonl'),'validation':os.path.join(zc_data_path,zc_task,'validation.jsonl')}) \n",
    "# download from huggingface\n",
    "data = load_dataset(\"tau/zero_scrolls\", zc_task)\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(data['test']))\n",
    "print(len(data['validation']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example=data['validation'][0]\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(example['input'])\n",
    "print(example['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_example = data['test'][0]\n",
    "tst_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tst_example['input'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we see that the test set includes 500 samples and the validation set includes 20 samples.  \n",
    "the difference between them is that the test set doesnt include the expected `output` (the value is `None`)\n",
    "\n",
    "next, lets see how to generate a prediction for the example. based on the code provided by zeroscrolls (`run_hf_model.py`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_max_input_tokens = {\n",
    "    \"google/flan-t5-xxl\": 8192,\n",
    "    \"google/flan-t5-xl\": 8192,\n",
    "    \"google/flan-t5-large\": 8192,\n",
    "    \"google/flan-t5-base\": 8192,\n",
    "    \"google/flan-t5-small\": 8192,\n",
    "    \"google/flan-ul2\": 8192,\n",
    "    \"bigscience/T0pp\": 8192,\n",
    "}\n",
    "\n",
    "def trim_doc_keeping_suffix(tokenizer, tokenized_input_full, example, suffix_index, max_tokens, device):\n",
    "    seperator_and_suffix = f\"{example['truncation_seperator'].strip()}\\n\\n{example['input'][suffix_index:].strip()}\\n\"\n",
    "    tokenized_seperator_and_suffix = tokenizer(seperator_and_suffix, return_tensors=\"pt\").input_ids.to(device)\n",
    "    tokenized_input_trimmed = tokenized_input_full[:, :max_tokens - tokenized_seperator_and_suffix.shape[1]]\n",
    "    tokenized_input = torch.cat([tokenized_input_trimmed, tokenized_seperator_and_suffix], dim=1)\n",
    "    return tokenized_input\n",
    "\n",
    "\n",
    "def process_model_input(tokenizer, example, max_tokens, device):\n",
    "    tokenized_input_full = tokenizer(example[\"input\"], return_tensors=\"pt\").input_ids.to(device)\n",
    "    if tokenized_input_full.shape[1] <= max_tokens:\n",
    "        return tokenized_input_full\n",
    "\n",
    "    seperator_and_query_text = example['truncation_seperator'] + example[\"input\"][example['query_start_index']:]\n",
    "    tokenized_seperator_and_query = tokenizer(seperator_and_query_text, return_tensors=\"pt\").input_ids.to(device)\n",
    "    input_without_query = example['input'][:example['query_start_index']]\n",
    "    tokenized_input_without_query = tokenizer(input_without_query, return_tensors=\"pt\").input_ids.to(device)\n",
    "    tokenized_input_without_query = tokenized_input_without_query[:,\n",
    "                                    :max_tokens - tokenized_seperator_and_query.shape[1]]\n",
    "\n",
    "    tokenized_input = torch.cat([tokenized_input_without_query, tokenized_seperator_and_query], dim=1)\n",
    "    return tokenized_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a model\n",
    "model_name='google/flan-t5-small'\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "max_input_length = model_to_max_input_tokens[model_name]\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name, device_map=\"auto\",\n",
    "                                                    torch_dtype=torch.float32)\n",
    "model = model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process an example\n",
    "model_input = process_model_input(tokenizer, example, max_input_length, device)\n",
    "prediction_token_ids = model.generate(model_input,\n",
    "                                        max_length=1024,\n",
    "                                        do_sample=False,\n",
    "                                        top_p=0,\n",
    "                                        top_k=0,\n",
    "                                        temperature=1)\n",
    "predicted_text = tokenizer.decode(prediction_token_ids[0], skip_special_tokens=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_token_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using GPT-style (decoder only) model\n",
    "in this section we'll try to use another model based on GPT. \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Hyena Checkpoint (134m) \n",
    "Now lets try to load the hyena checkpoint and see if we can feed the example in it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.sequence.long_conv_lm import ConvLMHeadModel\n",
    "from transformers import GPT2Tokenizer\n",
    "import yaml "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_hyena_model(model_cfg, ckpt_path):\n",
    "    config = yaml.load(open(model_cfg, 'r'), Loader=yaml.FullLoader)\n",
    "    model = ConvLMHeadModel(**config['model_config'])\n",
    "    state_dict = torch.load(ckpt_path, map_location='cpu')\n",
    "    model.load_state_dict(state_dict)\n",
    "    if config['tokenizer_name'] == 'gpt2':\n",
    "        tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "    else:\n",
    "        tokenizer = None \n",
    "    return model, tokenizer,config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cfg_file = 'configs/evals/hyena_small_150b.yaml'\n",
    "ckpt_path='checkpoints/hyena_small_150b_tok.ckpt'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = yaml.load(open(model_cfg_file, 'r'), Loader=yaml.FullLoader)\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmodel, htokenizer,config = load_hyena_model(model_cfg_file, ckpt_path)\n",
    "max_input_length = config['model_config']['layer']['l_max']\n",
    "\n",
    "hmodel = hmodel.to(device)\n",
    "hmodel = hmodel.eval()        # is it needed ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input = process_model_input(htokenizer, example, max_input_length, device)\n",
    "model_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = hmodel(model_input)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the kernel crashed when trying to run the inference.  \n",
    "to debug, check the format that is fed to the model with the Lambada dataset and compare to what you get here. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ZeroScrolls Long Context Team version\n",
    "The data was downloaded from a link provided to us by LC team. it is saved in the `datasets` folder.\n",
    "As for the scripts for evaluation, you can use the `zero_sc_eval` branch from NeMo and the scripts to run the benchmark are in the LC team gitlab repo.  \n",
    "\n",
    "in the following, I'll try to run their benchmark on the public model (flan-t5-small) we used above  \n",
    "\n",
    "They first generate the predictions, save it to a file and then they use the script to get the scrolls. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
