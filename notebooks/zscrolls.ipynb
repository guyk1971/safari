{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero Scrolls\n",
    "The goal of this notebook is to explore the zero scrolls dataset.  \n",
    "there are 2 versions: \n",
    "- public version \n",
    "- LC team's version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from transformers import set_seed as hf_set_seed\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sentencepiece accelerate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to run the hyena chckpoint, you need to install dropout_add_layer_norm from flash attention module\n",
    "# see this issue : https://github.com/HazyResearch/hyena-dna/issues/5\n",
    "\n",
    "# cd flash-attention/csrc/layer_norm\n",
    "# !pip install .\n",
    "# cd ../../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd flash-attention/csrc/fused_dense_lib\n",
    "# ! pip install ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ZeroScrolls public\n",
    "Benchmark [Homepage](https://www.zero.scrolls-benchmark.com/) and [github](https://github.com/tau-nlp/zero_scrolls/tree/main)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming the files have been downloaded and located in the `datasets` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zc_data_path='./datasets/zero-scrolls/public/'\n",
    "zc_tasks = os.listdir(zc_data_path)\n",
    "zc_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zc_task=zc_tasks[4]\n",
    "# load from  file\n",
    "# data = load_dataset('json',data_files={'test':os.path.join(zc_data_path,zc_task,'test.jsonl'),'validation':os.path.join(zc_data_path,zc_task,'validation.jsonl')}) \n",
    "# download from huggingface\n",
    "data = load_dataset(\"tau/zero_scrolls\", zc_task)\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(data['test']))\n",
    "print(len(data['validation']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example=data['validation'][0]\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(example['input'])\n",
    "print(example['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_example = data['test'][0]\n",
    "tst_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tst_example['input'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we see that the test set includes 500 samples and the validation set includes 20 samples.  \n",
    "the difference between them is that the test set doesnt include the expected `output` (the value is `None`)\n",
    "\n",
    "next, lets see how to generate a prediction for the example. based on the code provided by zeroscrolls (`run_hf_model.py`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_max_input_tokens = {\n",
    "    \"google/flan-t5-xxl\": 8192,\n",
    "    \"google/flan-t5-xl\": 8192,\n",
    "    \"google/flan-t5-large\": 8192,\n",
    "    \"google/flan-t5-base\": 8192,\n",
    "    \"google/flan-t5-small\": 8192,\n",
    "    \"google/flan-ul2\": 8192,\n",
    "    \"bigscience/T0pp\": 8192,\n",
    "}\n",
    "\n",
    "def trim_doc_keeping_suffix(tokenizer, tokenized_input_full, example, suffix_index, max_tokens, device):\n",
    "    seperator_and_suffix = f\"{example['truncation_seperator'].strip()}\\n\\n{example['input'][suffix_index:].strip()}\\n\"\n",
    "    tokenized_seperator_and_suffix = tokenizer(seperator_and_suffix, return_tensors=\"pt\").input_ids.to(device)\n",
    "    tokenized_input_trimmed = tokenized_input_full[:, :max_tokens - tokenized_seperator_and_suffix.shape[1]]\n",
    "    tokenized_input = torch.cat([tokenized_input_trimmed, tokenized_seperator_and_suffix], dim=1)\n",
    "    return tokenized_input\n",
    "\n",
    "\n",
    "def process_model_input(tokenizer, example, max_tokens, device):\n",
    "    tokenized_input_full = tokenizer(example[\"input\"], return_tensors=\"pt\").input_ids.to(device)\n",
    "    if tokenized_input_full.shape[1] <= max_tokens:\n",
    "        return tokenized_input_full\n",
    "\n",
    "    seperator_and_query_text = example['truncation_seperator'] + example[\"input\"][example['query_start_index']:]\n",
    "    tokenized_seperator_and_query = tokenizer(seperator_and_query_text, return_tensors=\"pt\").input_ids.to(device)\n",
    "    input_without_query = example['input'][:example['query_start_index']]\n",
    "    tokenized_input_without_query = tokenizer(input_without_query, return_tensors=\"pt\").input_ids.to(device)\n",
    "    tokenized_input_without_query = tokenized_input_without_query[:,\n",
    "                                    :max_tokens - tokenized_seperator_and_query.shape[1]]\n",
    "\n",
    "    tokenized_input = torch.cat([tokenized_input_without_query, tokenized_seperator_and_query], dim=1)\n",
    "    return tokenized_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a model\n",
    "model_name='google/flan-t5-small'\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "max_input_length = model_to_max_input_tokens[model_name]\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name, device_map=\"auto\",\n",
    "                                                    torch_dtype=torch.float32)\n",
    "model = model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process an example\n",
    "model_input = process_model_input(tokenizer, example, max_input_length, device)\n",
    "prediction_token_ids = model.generate(model_input,\n",
    "                                        max_length=1024,\n",
    "                                        do_sample=False,\n",
    "                                        top_p=0,\n",
    "                                        top_k=0,\n",
    "                                        temperature=1)\n",
    "predicted_text = tokenizer.decode(prediction_token_ids[0], skip_special_tokens=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_token_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using GPT-style (decoder only) model\n",
    "in this section we'll try to use another model based on GPT. \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Hyena Checkpoint (134m) \n",
    "Now lets try to load the hyena checkpoint and see if we can feed the example in it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.sequence.long_conv_lm import ConvLMHeadModel\n",
    "from transformers import GPT2Tokenizer\n",
    "import yaml "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_hyena_model(model_cfg, ckpt_path):\n",
    "    config = yaml.load(open(model_cfg, 'r'), Loader=yaml.FullLoader)\n",
    "    model = ConvLMHeadModel(**config['model_config'])\n",
    "    state_dict = torch.load(ckpt_path, map_location='cpu')\n",
    "    model.load_state_dict(state_dict)\n",
    "    if config['tokenizer_name'] == 'gpt2':\n",
    "        tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "    else:\n",
    "        tokenizer = None \n",
    "    return model, tokenizer,config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cfg_file = 'configs/evals/hyena_small_150b.yaml'\n",
    "ckpt_path='checkpoints/hyena_small_150b_tok.ckpt'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = yaml.load(open(model_cfg_file, 'r'), Loader=yaml.FullLoader)\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmodel, htokenizer,config = load_hyena_model(model_cfg_file, ckpt_path)\n",
    "max_input_length = config['model_config']['layer']['l_max']\n",
    "\n",
    "hmodel = hmodel.to(device)\n",
    "hmodel = hmodel.eval()        # is it needed ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input = process_model_input(htokenizer, example, max_input_length, device)\n",
    "model_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = hmodel(model_input)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the kernel crashed when trying to run the inference.  \n",
    "to debug, check the format that is fed to the model with the Lambada dataset and compare to what you get here. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ZeroScrolls Long Context Team version\n",
    "The data was downloaded from a link provided to us by LC team. it is saved in the `datasets` folder.\n",
    "As for the scripts for evaluation, you can use the `zero_sc_eval` branch from NeMo and the scripts to run the benchmark are in the LC team gitlab repo.  \n",
    "\n",
    "in the following, I'll try to run their benchmark on the public model (flan-t5-small) we used above  \n",
    "\n",
    "They first generate the predictions, save it to a file and then they use the script to get the scrolls. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
